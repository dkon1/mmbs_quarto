
\chapter[Global phase plane]{Global phase plane flow for nonlinear dynamical systems}
\section{Introduction}
Nonlinear dynamical systems can be devilishly difficult to comprehend. In most cases, one must abandon all hope for finding an explicit solution before entering this realm. The best that mathematics can offer is a qualitative understanding of the dynamics: will a solution approach a fixed point, or oscillate, or grow without bound. In the previous chapter, we learned to describe the qualitative behavior of solutions near the fixed points of nonlinear systems. Now we will develop the ideas and skills for putting together a global portrait of the flow of solution trajectories, even far from equilibria.

The main ideas  in this chapter serve to describe the geometry of solution trajectories in the phase plane. Compared with one variable dynamical systems, there are many possible qualitative behaviors of solutions in two dimension. Whereas on the line, a solution could fundamentally only either increase or decrease, and thus no oscillations are possible, solutions in the phase plane could approach a fixed point, approach infinity, or oscillate. We will use the mathematical theory of existence and uniqueness of solutions to define different regions of the phase plane, and to describe the fate of different solutions, depending on the initial conditions.

In the modeling section, we return to the notion of potential functions and examine how they result in special quantities that remain fixed over the course of a solution, or conserved quantities. We also touch upon the notions of potential energy wells and barriers, and describe their use in modeling chemical kinetics. In the analytical section we build up the theory of dynamical systems in order define concepts of basins of attraction and of special trajectories that serve as their boundaries. Then we use these ideas to give a comprehensive analysis of  potential-based systems. In the numerical section, we present our first high-order method of numerical solution, called the midpoint method. The synthesis section we analyze the classic model of predator-prey interaction, called the Lotka-Volterra model, and demonstrate the geometry of its phase plane flow in terms of the notions developed in the preceding sections.

\section{Modeling: potential landscapes and conserved quantities}

\subsection{potential functions and energy}
In chapter 7 we introduced a special kind of dynamical systems generated by potential functions. The potential functions produce a force that defines the dynamics of the system. The force in these systems is defined to be the negative derivative of the potential $V(x)$, where $x$ is the one-dimensional variable: $f(x) = - V'(x)$. More generally, if the variable has multiple dimensions, or equivalently there are multiple variables, the potential function depends on a vector, $V(\vec x)$, although it still returns a scalar value. Then the force is given by the negative gradient of the potential function, which is the vector of partial derivatives with respect to all $n$ variables:
$$ \vec f(\vec x) = - \left(\frac{\partial V}{\partial x_1} , ... , \frac{\partial V}{\partial x_n} \right) =  -\nabla V(\vec x)  $$ 
Newton's second law yields the following differential equation, known as the \emph{equation of motion} for the dynamical system:
$$ m \frac{d^2 \vec x}{dt^2}  = -\nabla V(\vec x) $$

In this chapter, we will restrict the discussion to potential functions of one variable, and thus will not need the gradient notation. However, because the equation of motion is a second-order differential equation, there are effectively two variables that one needs to consider, in order to specify the state of the system: $x$ and its time derivative $\dot x$. In physics, these describe the position and velocity of a particle, respectively. Introducing a new variable $y = \dot x$, we obtain two first-order ODEs in standard, autonomous form:
\begin{eqnarray*}
\frac{d  x}{dt}  &=& y \\
\frac{d y}{dt}  &=& -\frac{1}{m}\frac{d V(x)}{d x}
\end{eqnarray*}
 
Due to their special form, these equations generate a particular kind of dynamical system. These dynamical systems are defined by the potential function, and are known as \emph{Hamiltonian} systems in classical mechanics, after the mathematician Hamilton. The most important special property of Hamiltonian systems is that they possess a quantity which is conserved, that is, does not change over time. This can be shown by multiplying the second-order equation of motion by $\dot x$ and integrating:
$$ m \ddot {\vec x }\dot{ \vec x} + \nabla V(\vec x) \dot{ \vec x}  = 0 \Longrightarrow \frac{m}{2} \dot x^2  + V(x) = E$$
This conserved quantity is a sum of the potential and the kinetic energy of the system, and is called the \emph{ total energy} of the system. The total energy $E$ is set by the initial conditions, e.g. how far the spring of a harmonic oscillator was initially displaced. Over time, the energy transforms between potential to kinetic, but it never dissipates. This leads to special properties of the solutions, which we will investigate in the analytical section.

\textbf{Example: harmonic oscillator with damping.} In Chapter 7, we saw the case of the linear oscillator, with a linear restoring force. The model is based on  a simple potential function $V(x) = k x^2/2$, which generates the following equation of motion:
$$m\ddot x=- \frac{d V(x)}{dx} =-kx $$
As we saw before, the fixed point at the bottom of the potential energy well $x^*=0$ is a center, with solutions circulating in periodic orbits in the phase plane of position and velocity. Also, we can find the conserved quantity, or energy of the harmonic oscillator: $E=\frac{m}{2}\dot x^2 + \frac{k}{2}x^2$.

Now let us add linear damping for this oscillator, proportional to the velocity $\dot x$: $m\ddot x= -kx - \mu \dot x $. We can re-write it as follows: $\mu \dot x = -m\ddot x - kx$. Let us examine how the energy changes over time in the damped case:
$$ \frac{dE}{dt} = \frac{m}{2}\dot x \ddot x + \frac{k}{2}x \dot x = \frac{\dot x}{2} (m\ddot x + kx) = -\frac{\mu \dot x^2}{2} $$
Therefore, for $\mu >0$, the potential energy always decreases with time for any solution; it is dissipated by the damping force. Since the potential energy function is very similar to distance from the origin, as it decreases, the solution approaches the origin. As we saw before, this leads to a stable spiral if the system is underdamped, or a stable node (exponential decay) if it is overdamped. 

\subsection{biochemical reaction kinetics and energy barriers}
One common application of potential energies is the biophysical models of molecular reactions. A reaction is thought of as transition from one steady state to another, which is modeled using potential energy functions as a transition from one energy well to another (termed \emph{reactant} and \emph{product} wells), in the process crossing the energy barrier, or the saddle point. 

The energy barrier is of paramount importance for \emph{reaction kinetics}, that is, the rate of transition between reactants and products. It turns out that the rate of reaction is proportional to the exponential of the barrier height divided by the temperature $T$ times the Boltzmann constant $k_B$:
$$ k_{rxn} \propto e^{\Delta V/(k_B T)} $$
This expression is derived from equilibrium statistical mechanics, where there is a constant temperature in the system that supplied random kicks to molecules, occasionally propelling them over the energy barrier. The derivation is outside of the scope of this class. 

The point is that the height of the energy barrier is key to finding biochemical reaction kinetics. Find the height of the barrier, and the whole energy landscape for large biological molecules (e.g. proteins, RNA, sugars) is daunting, due to the thousands of atoms and corresponding degrees of freedom. Finding the pathway between two equilibrium states (the wells) over highly multidimensional energy landscapes is a very challenging computational problem, for which a description of the physical forces is needed to compute the molecular dynamics trajectories.

\section{Analytical: global phase portraits}
We will now analyze the global dynamics of multivariable dynamical systems, in terms of the geometry of the flow of the solution trajectories in the phase plane. We will restrict the discussion to two variables, because we can still visualize a phase plane on a sheet of paper, and because the dynamics with more variables can be exceedingly complicated. First we will define some general concepts for describing different regions of a phase plane, and then we will apply those to analyzing several systems, with special consideration for systems with a conserved quantity.

\subsection{basins of attraction and special trajectories}
In Chapter 3 we first introduced the concepts of existence and uniqueness of solutions for one-variable differential equations. You will recall that it was required that the function that defines the derivative was smooth and differentiable, in order to guarantee the existence of a unique solution for a specified initial condition. We will now use this existence and uniqueness of solutions to describe a fundamental property about solutions in phase space: trajectories cannot intersect anywhere except at fixed points.

\begin{theorem}
Suppose that $ \dot{\vec x} = f(\vec x)$ is a multi-variable dynamical system, with $f$ an $N$-dimensional function which is smooth and has partial derivatives in all $N$ variables.  Then for any point $\vec x_0$, such that $f(\vec x_0) \neq 0$, there exists one and only one solution trajectory which passes through this point.
\end{theorem}

The idea behind the proof is fairly intuitive - the function $f(\vec x_0$) supplies the derivative, or the tangent vector to the solution trajectory at the point $\vec x_0$, and there cannot be two different curves passing through the same point with the same tangent vector. This is proven rigorously in what is called the Implicit Function Theorem, and this existence and uniqueness theorem follows. However, notice that the theorem does not apply to the points where the derivative function is zero, i.e. the fixed points. At the fixed points there is no flow, and thus no direction is specified. Instead, multiple trajectories meet at fixed points, either converging toward a stable fixed point, or diverging away from an unstable one. This does not contradict the existence and uniqueness of solutions, since it takes an infinite time for a solution to actually reach a fixed point, and thus there is still no intersection of different solution trajectories in finite time.

Paraphrasing Gaugin, we can ask of solution trajectories: ``where do you come from? where are you going?'' We have shown that for a well-behaved dynamical system, there is a single trajectory passing through any non-fixed point in phase space. This unique solution can be extended as far as desired, both forward and backward in time. The points (or sets of points) that solutions come from are called \emph{sources} or \emph{repellers}, while the points (or sets of points) that solutions tend to are called \emph{sinks} or \emph{attractors}. For now, we will restrict our conception of sources and sinkls to fixed points, but in the next chapter we will see that there are other objects that solutions can approach. It should be noted that not all trajectories approach a specific point or set of points, either in forward or reverse time: they can also go off to infinity.
\begin{mosdef}
Suppose that $\vec x^*$ is a stable fixed point of a multivariable dynamical system. The \emph{basin of attraction} of that fixed point is the set of all points in phase space, such that the trajectory which passes through this point is attracted to $\vec x^*$, that is, it approaches the fixed point for all time.  
\end{mosdef}

The basins of attraction are well-defined, connected regions of space, due to the existence and uniqueness of solutions. First, a trajectory passing through a given point can only have one destination. Second, all of the intermediate points in a trajectory on the way to the destination must also be in the basin of attraction, so it would be impossible to have a basin of attraction with two disconnected regions, since the trajectories all have to end up in the same place. 

Dividing phase space by basins of attraction is important for predicting the fate of a dynamical system model. In many nonlinear dynamical systems there are several stable fixed points, and the fundamental question is which one will a particular solution approach. The concept of basins of attraction, based on the principle of uniqueness of solutions, allows to determine the long-term dynamics based only on the location of the initial value. In order to specify  the boundaries of basins of attraction, we define a new concept.
\begin{theorem}
Consider a two-variable dynamical system with a smooth and differentiable flow, with more than one stable fixed point. On the boundary between any two basins of attraction there exists a unique trajectory which is not attracted to either stable fixed point, called a \emph{separatrix}, such that any deviation from it lies in one of the two basins of attraction. 
\end{theorem}
The proof of this theorem proceeds by contradiction. Suppose there were no such boundary between basins of attraction, then we could find two points in the phase plane arbitrarily close to each other, but which belong to different basins of attraction. Thus, we can find two points which are infinitesimally close, but have substantially different directions of flow, as specified by the defining function. This contradicts the smoothness assumption for the dynamical system. Instead, there must be a boundary, on which the direction of flow is toward neither fixed point, but instead flows in a ``perpendicular'' direction.  

\textbf{Example: rabbits and sheep, cont.} 
In the last chapter we determined the local behavior of the ecological competition model  around four different fixed points. Since this dynamical system is smooth and differentiable,  we can extend this picture to the whole phase plane. Figure \ref{fig:rs_nullclines} shows the geometry of the nullclines and the global flow in the phase plane of this model. The direction of flow on the nullclines is vertical for those with $\dot x = 0$ (shown in pink) and horizontal for those with $\dot y = 0$ (shown in orange). The intersections of the nullclines of different colors are the fixed points. It can be seen geometrically that the flow near the fixed points corresponds to the analysis we performed above.
 \begin{figure}[htbp]
   \centering
   \includegraphics[width=4.5in]{fig_ch10/week8_fig1.png} 
     \caption{The flow, nullclines (pink for $\dot x  = 0$; orange for $\dot y = 0$) and fixed points (stable marked in blue, unstable marked in green) for the rabbit-sheep coexistence model. Separatrix between the two stable fixed points sketched in red dashed lines.}
   \label{fig:rs_nullclines}
\end{figure}
This picture of phase plane flow can help us answer the basic dynamics question, where do solutions end up after a long time? There are two stable fixed points. So, for this model, all solutions will tend either to only rabbits (3,0) or only sheep (0,2). Looking at the flow, it is evident that the phase plane is divided into two basins of attraction: solutions that start (roughly) to the right of the coexistence fixed point at (1,1) end up at (0,2), while those that start to the left of (1,1) end up at (3,0). 

There is a separatrix that divides the two basins of attraction, and it lies along the direction of the stable eigenvector near the saddle point at (1,1). Consider the flow leaving the origin in the positive quadrant. It can approach either (3,0) or (0,2), and since the flow cannot jump discontinuously, there has to be a theoretical trajectory which leaves the origin and does not tend to either stable fixed point, but instead goes to the coexistence fixed point (1,1). This is a heteroclinic trajectory which connects two different unstable fixed points: the unstable node at the origin and the saddle point; it originats at the unstable eigenvector of the fixed point at the origin and ends at the stable eigenvector of the saddle point, and it serves as the separatrix between the two basins of attraction, shown as a red dashed line in figure \ref{fig:rs_nullclines}. There is also a separatrix on the other side of the saddle point, which is a trajectory that approaches the point from infinity, also shown in a red dashed line. Near the fixed point it follows the direction of the stable eigenvector of  the saddle point. The division of the phase plane allows us to answer the global question: given a particular initial condition, will the solution approach the all-sheep or all-rabbits equilibrium.

\subsection{properties of conservative dynamical systems}
In the modeling section we discussed dynamical systems generated by a potential function, known in classical mechanics as a Hamiltonian system. The dynamics of solutions is governed by the potential function, since the equation of motion is defined by the gradient of the potential. Thus, the fixed points of  the system are located at the points where the gradient of the potential vanishes, for instance the maxima and minima of the function. In general, the potential function provides an intuitive description of the dynamics: one can visualize a marble rolling on the surface of the potential function, starting at a given position with a given initial velocity, and then rolling around subject to a downward (e.g. gravitational) force. This intuition will be formalized below to describe the general properties of such dynamical systems.
 
We saw in the modeling section that a potential-derived dynamical system must possess a conserved quantity. This quantity is determined by the initial condition, and then stays constant over the entire time course of each trajectory. This fact leads to an important general statement about conservative systems: they cannot have any purely attracting (or repelling) fixed points. To prove this statement rigorously, we must specify that the potential system has a conserved quantity which is not constant over any region (open set, mathematically speaking), which excludes boring dynamical systems like $\dot x = \dot y = 0$. 
\begin{theorem} A dynamical system with a conserved quantity, which is not constant over any open set, cannot have any attracting fixed points. 
\end{theorem}

\textbf{Proof.} By contradiction: assume that there is a stable fixed point. All trajectories that lead to it have the same energy as the fixed point. This means that the conserved quantity is constant in the entire basin of attraction of the fixed point. Mathematical analysis tells us that a basin of attraction must be an open set. Thus, we have a contradiction.

The theorem proves that in a (nontrivial) conservative system there can be no stable fixed points, and if we reverse the direction of time in the argument,  we can demonstrate that there can be no purely unstable (repelling) fixed points either. In fact, a conservative system can have only fixed points that are neither purely attracting nor purely repelling, such as saddles and centers.  However, if damping is added, as we saw in the harmonic oscillator example in the modeling section, trajectories tend toward the stable fixed point of the resting position. This is because adding damping results in dissipation of energy over the time course of a trajectory, and a trajectory can approach a potential energy minimum, which is stable fixed point in a potential system with damping. Thus, a marble that rolls inside a potential wells will gradually approach the potential minimum, either in oscillatory fashion (underdamped) or monotonically (overdamped). In summary, adding damping to a conservative system transforms the fixed points which are centers into stable spirals, but does not change the nature of the saddle fixed points.

This leads to a very particular kind of phase plane portrait for any conservative system with a potential function $V(x)$. Each local minimum of $V(x)$ is a center point, with closed trajectories around it indicating oscillations within the particular potential energy well. Each local maximum is a barrier which separates two wells, and is a saddle point in the phase plane.  It should be noted that the center points are not actually stable, and rather can be considered neutrally stable. Still, there is a qualitative shift in the dynamics around the center points, where there are closed trajectories, and away from them, where the trajectories between positive and negative infinity. Applying the intuition of marbles rolling on a potential landscape, given a small kick, a marble will keep rolling back and forth within a single a well. However, given enough initial velocity ($y$ in the phase plane) the marble will roll out of any finite-sized well, and roll down toward infinity, unless the potential function itself goes to infinity for $x \to \pm \infty$.

The closed trajectories inside each energy well are bounded by separatrices around each basin of attraction. These are called \emph{homoclinic} trajectories, which both originate and tend toward the same saddle point. Using the marble analogy, they correspond to a marble gently rolling off the top of a hill, reaching the same height on the opposite side of the well, and then returning to the initial position at the top of the hill. In the phase plane, the homoclinic trajectory leaves the saddle point along the unstable direction, and returns along the stable direction. Notice that as in the example above, saddle points are key players at separating different regions of the phase plane.

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=5in]{fig_ch10/lec14_fig1.png} 
   \caption{The potential energy function $V(x) = -1/2 x^2 +1/4 x^4$ and its fixed points.}
   \label{fig:pot_fun}
\end{figure}


Thus, given a graph of a potential function $V(x)$, we now have the means to sketch the flow of the dynamical system, with the variables of $x$ and $\dot x$, e.g. $x$ representing position and $\dot x$ representing velocity. Inside each potential well, there are closed oscillatory trajectories, which are separated by unique closed trajectories, originating and ending at each potential maximum. The phase portrait for a potential function with multiple minima resembles funky glasses for a creature with multiple eyes. The fate of trajectories outside of the influence of each potential well, whether by position, or velocity, depends on the behavior of the potential function $V(x)$ when $x$ is very large, or very negative. If the potential increases without bound  on both ends, then no trajectories can grow without bound; conversely, if the potential decreases to negative infinity on both sides, then trajectories ``fall'' into the bottomless well toward infinity. It is possible for a potential function to approach infinity on one end and negative infinity on the other, in which case the behavior for trajectories will depend on where they originate. In another possibility, a potential function may be infinitely periodic, e.g $\sin(x)$, in which the local phase portrait will be repeated indefinitely. Finally, a potential function may approach an asymptote as $x \to \pm \infty$, and thus the dynamical system approaches a fixed point at infinity. In this scenario, given enough velocity a trajectory will grow indefinitely, but with insufficient momentum the trajectory will be closed, and keep oscillating.

\textbf{Example.} Let us consider the potential $V(x) = -1/2 x^2 +1/4 x^4$. The ODEs for the position ($x$) and velocity ($\dot x$) equations of motion are:
\begin{eqnarray*}
\dot x &=& y \\
\dot y &=& x-x^3
\end{eqnarray*}

The fixed points are found at $y = 0, x=0,\pm 1$. Let us analyze the fixed points via linearization, by finding the general Jacobian matrix:
$$ J(x,y) = \left(\begin{array}{cc}0 & 1 \\1-3x^2 & 0\end{array}\right)$$
For the three fixed points, the matrices are:
\begin{description}
\item [at $(0,0)$]
$$ J(0,0) =  \left(\begin{array}{cc}0 & 1 \\1 & 0\end{array}\right)$$
The eigenvalues are: $\lambda = \pm 1$, so this fixed point is a saddle;
\item  [at $(\pm 1, 0)$]
$$ J(\pm 1,0) = \left(\begin{array}{cc}0 & 1 \\-2 & 0\end{array}\right)$$ 
The eigenvalues are: $\lambda = \pm \sqrt{2} i$, so these fixed points are centers.
\end{description}
The conclusions are summarized in figure \ref{fig:pot_fun}. 


%\section{Computational: Higher order numerical methods}
%In Chapter 3 we introduced simple finite-difference schemes for numerical solution of differential equations. The forward Euler and backward Euler possess the virtue of simplicity, but they suffer in the realm of accuracy. Intuitively, this means that in order to reduce the error, the time step has to be very small. There are other, more complicated methods, which produce better accuracy for the same time step. To make this distinction, we define the order accuracy of a numerical method:

%\begin{mosdef}
%Suppose that a dynamical system has exact solution $x(t)$, and a numerical scheme with time step $\Delta t$ produces an approximate solution $y(t)$. It has \emph{local error} $\epsilon(\Delta t) = |x(t_0+\Delta t) - y(t_0+\Delta t)|$, starting at some time $t_0$ and taking one time step. If the local error is described by the power law $\epsilon(\Delta t) \propto \Delta t ^{n+1}$ as $\Delta t \to 0$ for any starting point $t_0$, then $n$ is the \emph{order of accuracy} of the numerical scheme.
%\end{mosdef}

%The order of a method is defined by the power of the dependence of the error accumulated over a single time step, in the limit of a small time step. That is, if for a 2-fold reduction in step size, the local error is reduced 4-fold, the method is first order in accuracy, but if the same reduction leads to a 32-fold improvement in local error, the method has fourth order accuracy. In practice, however, the overall error of a simulation is a sum of all the local errors accumulated over all $N$ time steps, and so the total error is approximately $N \epsilon(\Delta t) $. For a given time $T$ of a simulation and a given time step $\Delta t$, the number of time steps is found by $N = T/\Delta t$, and so the total error is proportional to $\epsilon(\Delta t)/\Delta t$, and the power law dependence for small time step has power $n$ instead of $n+1$. Thus, for a fourth order method, reducing the step size 2-fold leads to a reduction of total error by a factor of 16, due to accumulation of local errors.

%\textbf{Example: order of forward Euler method.} 
%The forward Euler scheme for a one-dimensional dynamical system $\dot x = f(x)$ is given by
%\begin{equation}
% y_{n+1} = y_n + \Delta t f(y_n) + O(\Delta t^2)
%\label{eq:euler_meth}
%\end{equation}
%where $y_n = y(n\Delta t)$ is the approximate solution after $n$ time steps, and $O(\Delta t^2)$ denotes terms proportional to $\Delta t^2$ and higher powers. This is derived from the first order approximation of the solution around a time $t_0$: $x(t_0+ \Delta t)  = x(t_0) + \Delta t f(x(t_0)) + O(\Delta t^2)$. The local error can be calculated from this, assuming that at time $t_0$ both solutions had the value $x_0$:
%$$\epsilon (\Delta t) = |x(t_0+ \Delta t)- y(t_0+\Delta t)| = | \Delta t f(x_0) - \Delta t f(x_0) + O(\Delta t^2)| = O(\Delta t^2) $$
%We see that terms of order 2 are left over in the error, and thus forward Euler is a first order method. In order to reduce the total error by a factor of 10, the time step has to be reduced by a factor of 10, and thus 10 times as many time steps are needed to simulate over the same time.

%\subsection{midpoint method}

%\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
%   \centering
 %  \includegraphics[width=3in]{fig_ch10/midpoint_method_illustration.png} 
 %  \caption{Illustration of the midpoint method, with $y(t)$ representing the exact solution of the differential equation, and $y_n$ and $y_{n+1}$ the values of the numerical solution at times $t_n$ and $t_{n+1}$, with timestep $h$. Taken from   \url{http://en.wikipedia.org/wiki/Midpoint_method}.}
%   \label{fig:midpoint_method}
%\end{figure}

%We now introduce a method with a higher order of accuracy. Using the forward Euler method, let us calculate the value of the solution after half a time step $y_{n+1/2} = y_n  +  \frac{\Delta t}{2}f(y_n)$, and then use this midpoint approximation to plug into the derivative function $f(y_{n+1/2})$.  This numerical scheme is called the \emph{midpoint method}:
%\begin{equation}
% y_{n+1}  = y_n + \Delta t f(y_n + \frac{\Delta t}{2} f(y_n))
%\label{eq:midpoint_meth}
%\end{equation}
%This is illustrated graphically in figure \ref{fig:midpoint_method}. This scheme mitigates the error in the Euler method by taking a smaller time step and using that midpoint value to estimate the slope of the solution, rather than the value at the beginning of the interval in Euler's method. To calculate the order of accuracy of the midpoint method, let us use a first-order expansion for the function $f(y_n + \epsilon) = f(y_n) + \epsilon \frac{d f}{dy}(y_n) + O(\Delta t^2)$ and re-write the midpoint scheme like this:
% $$ y_{n+1}  = y_n + \Delta t \left(f(y_n) +  \frac{\Delta t}{2} f(y_n) \frac{d f }{dy}(y_n)+ O(\Delta t^2) \right) = y_n + \Delta t f(y_n) + \frac{\Delta t^2}{2} f(y_n) \frac{d f}{dy}(y_n)+ O(\Delta t^3)  $$
%To find the local error, the derivative function needs to be expressed in terms of the solution function $x(t)$. We know that $dx/dt = f(x)$. By using the chain rule, we determine that
 %$$ \frac{df}{dx} f(x) =  \frac{df}{dx} \frac{dx}{dt} = \frac{df}{dt} = \frac{d^2x}{dt^2}$$
%Substituting this into the above expression for the midpoint scheme, we obtain:
%$$ y(t_{n}+\Delta t)  = y(t_n) + \Delta t \frac{dy}{dt} (t_n)+ \frac{\Delta t^2}{2} \frac{d^2y}{dt^2}(t_n) + O(\Delta t^3) $$
%Notice that this is precisely the first three terms of the Taylor expansion, which is the same for the exact solution $x(t)$:
%$$ x(t_{n}+\Delta t)  = x(t_n) + \Delta t \frac{dx}{dt} (t_n)+ \frac{\Delta t^2}{2} \frac{d^2x}{dt^2}(t_n) + O(\Delta t^3) $$
%Therefore, the local error $\epsilon (\Delta t) = |x_{n+1} - y_{n+1}| = O(\Delta t^3)$ and the method has second order accuracy. The midpoint method is more computationally costly than the forward Euler method, since it requires two function evaluations to determine the next time step, as shown in equation \ref{eq:midpoint_meth}, instead of a single function evaluation, as seen in equation \ref{eq:euler_meth}. The payoff is an increase in the order of accuracy from 1 to 2, which may save much more computation by allowing the step size to remain larger.


%\begin{algorithm}
%\caption{Pseudocode for forward Euler's method }
%\begin{algorithmic}
%\STATE define the derivative function for the ODE $f(x,t)$
%\STATE set the step size $\Delta t$ and number of iterations $Niter$
%\STATE set initial values $x[1]$ and $t[1]$
%\STATE initialize two arrays $t$ and $x$ with length $Niter$
%\FOR {$ i=1$ to $Niter$}
%      \STATE  $x[i+1] \gets x[i] + \Delta t* f(x[i], t[i])$ 
%     \STATE $t[i+1] \gets t[i] +\Delta t$  
%\ENDFOR             
%\end{algorithmic}
%\label{alg:forward_euler}
%\end{algorithm}

\section{Synthesis: Lotka-Volterra predator-prey model}
In this section we will consider a classic ecological model with two variables, representing the populations of species of predator and its prey. In order to construct the model, we first assume that the prey species will grow without bound if left to the own devices, while the prey would die out. This assumption is simplistic, but it makes some sense unless the populations get too large. In addition, the two species influence each other via interaction terms. Naturally, the interaction has a positive effect on the rate of growth of the predator, and a negative effect on that of the prey. As before, the simplest way of modeling that interaction is with a product term of the two populations, so the greater numbers of prey, the easier it is for a predator to catch one, and the greater the numbers of predators, the easier it is for prey to get caught. This results in the classic Lotka-Volterra model:
\begin{eqnarray*}
\dot x&=&  \alpha x -\gamma xy\\
\dot y &=&  -\beta y + \delta  xy 
\end{eqnarray*}
Here $x$ and $y$ are the numbers of prey and predators, respectively, and the other symbols are positive parameters: $\alpha$ is the inherent growth rate of the prey, and $\beta$ is the inherent death rate of the predator; while $\gamma$ quantifies the effect of predation on the prey, while $\delta$ does the same for the effect of predation on the predator.

Let us first perform linear stability analysis of the fixed points  of the dynamical system. The nullclines for $x$ are $x = 0$ and $y = \alpha/\gamma$;  the nullclines for $y$ are $y = 0$ and $x = \beta/\delta$. Thus, there are two fixed points: $(0,0)$ (both species are extinct) and $( \beta/\delta, \alpha/\gamma)$ (coexistence). To determine their stability, we find the Jacobian of the system and plug in the values of  the fixed points:
$$ J(x,y) = \left(\begin{array}{cc} \alpha - \gamma y & -\gamma x \\ \delta y  & -\beta + \delta  x \end{array}\right)$$
At the extinction equilibrium, the Jacobian is:
$$J(0, 0) =  \left(\begin{array}{cc} \alpha &0 \\ 0  & -\beta \end{array}\right)$$
The eigenvalues are plainly $\lambda = \alpha$ and $\lambda = \beta$, and since the parameters are positive, the fixed point is a saddle. This means, that is stable along the predator axis, since the predators die out if left to themselves, but unstable along the prey axis, since prey grow unchecked without predators. 

At the coexistence equilibrium, the Jacobian is:
$$J( \beta/\delta, \alpha/\gamma) =  \left(\begin{array}{cc} 0&  -\gamma \beta/\delta \\ \delta \alpha/\gamma & 0 \end{array}\right) $$
The eigenvalues are $\lambda = \pm \sqrt{\alpha \beta }i $, indicating oscillatory solutions.  In general, eigenvalues with real part of 0 are not rubust, and must be carefully scrutinized, because the presence of nonlinear terms can push them either toward the fixed point (stability) or away (instability). Geometrically speaking, trajectories that go around the fixed point must return to the initial location, in order for solutions to be truly periodic. 

Indeed, oscillatory solutions prevail in the Lotka-Volterra model, despite its nonlinear nature. The dynamical system turns out to be conservative, although it is not based on a potential function. It possesses a conserved quantity with the following form:
$$ C = \alpha \log y - \gamma y  -  \delta  x + \beta \log x$$
I leave the verification of conservation of this expression to the reader. The existence of the conserved quantity makes closed solution trajectories possible. The constant $C$ is specified by the initial numbers of the predators ($y$) and prey ($x$) and then remains the same along each solution trajectory. The model predicts alternating cycles of large numbers of predators and prey, which are sometimes observed in nature.  The geometry of  flow in the phase plane is summarized in figure \ref{fig:lot_vol}, showing the straight-line nullclines and the cyclical solution trajectories.

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=4.5in]{fig_ch10/lot_vol_pp.png} 
   \caption{Phase plane flow in Lotka-Volterra model}
   \label{fig:lot_vol}
\end{figure}
