<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Mathematical Methods for Biology, Part 1 - 7&nbsp; Linear regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./ch8_LinReg_python.html" rel="next">
<link href="./ch6_matrix_mult.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Linear regression</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Mathematical Methods for Biology, Part 1</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch1_discrete1var.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">One variable in discrete time</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch2_plotting_python.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Plotting in Python</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch3_discrete_chaos.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Nonlinear discrete-time dynamic models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch4_cobweb_plots.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Graphical analysis of difference equations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch5_discrete_higher.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Discrete models of higher order</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch6_matrix_mult.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Matrix multiplication and population models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch7_linear_reg.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Linear regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch8_LinReg_python.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Linear regression in Python</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch9_1var_ode.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Models with one variable in continuous time</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch10_numeric_ode.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Numeric solutions of ODEs</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch11_graph_ode.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Graphical analysis of ordinary differential equations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch12_linear_pplane.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear ODEs with two variables</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch13_phase_portraits.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Phase portraits in Python</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch14_lin_oscillations.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Forces and potentials in biological modeling</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch15_fourier.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Fourier series: decomposition by frequency</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch16_linearization.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Linearization of ODEs</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch17_nonlinear_oscillations.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Nonlinear oscillations in biology</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#list-of-terms-and-concepts" id="toc-list-of-terms-and-concepts" class="nav-link active" data-scroll-target="#list-of-terms-and-concepts"><span class="toc-section-number">7.0.1</span>  List of terms and concepts</a></li>
  <li><a href="#systems-of-linear-equations" id="toc-systems-of-linear-equations" class="nav-link" data-scroll-target="#systems-of-linear-equations"><span class="toc-section-number">7.1</span>  Systems of linear equations</a>
  <ul class="collapse">
  <li><a href="#invertibility-of-matrices" id="toc-invertibility-of-matrices" class="nav-link" data-scroll-target="#invertibility-of-matrices"><span class="toc-section-number">7.1.1</span>  invertibility of matrices</a></li>
  </ul></li>
  <li><a href="#fitting-a-line-to-data" id="toc-fitting-a-line-to-data" class="nav-link" data-scroll-target="#fitting-a-line-to-data"><span class="toc-section-number">7.2</span>  Fitting a line to data</a>
  <ul class="collapse">
  <li><a href="#minimizing-the-sum-of-residuals" id="toc-minimizing-the-sum-of-residuals" class="nav-link" data-scroll-target="#minimizing-the-sum-of-residuals"><span class="toc-section-number">7.2.1</span>  minimizing the sum of residuals</a></li>
  </ul></li>
  <li><a href="#assumptions-of-linear-regression" id="toc-assumptions-of-linear-regression" class="nav-link" data-scroll-target="#assumptions-of-linear-regression"><span class="toc-section-number">7.3</span>  assumptions of linear regression</a></li>
  <li><a href="#linear-least-squares-for-polynomial-fitting" id="toc-linear-least-squares-for-polynomial-fitting" class="nav-link" data-scroll-target="#linear-least-squares-for-polynomial-fitting"><span class="toc-section-number">7.4</span>  linear least squares for polynomial fitting</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Linear regression</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>One of the most common ways of either fitting data, or if you want to put it in a fancier way, train a machine learning model, is called linear regression. This starts with a data set that has two different variables and pairs of observations of each, and produce a linear model that uses one variable (called explanatory) to predict the other (called response). Graphically speaking, the goal is to plot a line on a scatterplot that best fits the data (in one variable).</p>
<p>Though it is generally not possible to produce an exact fit for more than two observations, there is a method to calculate the closest linear model, called least-squares fitting. We will develop some fundamental tools from linear algebra to do this calculation, and then talk about the underlying assumptions and what they mean for applicability of linear regression.</p>
<section id="list-of-terms-and-concepts" class="level3" data-number="7.0.1">
<h3 data-number="7.0.1" class="anchored" data-anchor-id="list-of-terms-and-concepts"><span class="header-section-number">7.0.1</span> List of terms and concepts</h3>
<ul>
<li>Solving linear equations</li>
<li>Matrix inverse</li>
<li>Least-squares data fitting</li>
<li>Explanatory vs.&nbsp;response variables and supervised learning</li>
<li>Covariance and correlation</li>
<li>Goodness of fit and R-squared</li>
<li>Polynomial regression</li>
<li>Residuals and assumptions of linear regression</li>
</ul>
</section>
<section id="systems-of-linear-equations" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="systems-of-linear-equations"><span class="header-section-number">7.1</span> Systems of linear equations</h2>
<p>As one goes through life, sometimes one has to solve a set of linear equations, that have multiple variables (let’s call them <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>) and the same number of equations that they need to satisfy with constant coefficients. For example, here is a system of two linear equations:</p>
<p><span class="math display">\[
2a - b = -3 \\
a + b  = 1
\]</span></p>
<p>where we want to find <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> that satisfy both equations. This can be written as a matrix equation, with matrix <span class="math inline">\(M\)</span> containing the coefficients on the left hand side and the vector <span class="math inline">\(\vec v\)</span> containing the two coefficients on the right hand side, and the vector <span class="math inline">\(\vec a\)</span> containing the unknown variables <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>:</p>
<p><span class="math display">\[
\begin{pmatrix} 2 &amp; -1 \\ 1 &amp; 1 \end{pmatrix} \times \begin{pmatrix} a \\ b\end{pmatrix} = \begin{pmatrix}-3 \\1\end{pmatrix} \\
M \vec a =  \vec v
\]</span></p>
<p>Written as a single linear equation, it is tempting to “divide” both sides by <span class="math inline">\(M\)</span> and thus solve for the vector <span class="math inline">\(\vec a\)</span>, but matrices cannot be reciprocated like numbers. Linear algebra provides a way of doing this correctly.</p>
<p>In order to get rid of the matrix <span class="math inline">\(M\)</span> on one side of the equation, one can multiply it by another matrix called its inverse.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<p>For a square (<span class="math inline">\(n\)</span> by <span class="math inline">\(n\)</span>) matrix <span class="math inline">\(M\)</span> the <em>inverse</em> matrix <span class="math inline">\(M^{-1}\)</span> (also <span class="math inline">\(n\)</span> by <span class="math inline">\(n\)</span>) satisfies the following conditions: <span class="math inline">\(M^{-1} \times M = M \times M^{-1} = I\)</span>, where <span class="math inline">\(I\)</span> is the <span class="math inline">\(n\)</span> by <span class="math inline">\(n\)</span> identity matrix.</p>
</div>
</div>
<p>Example: For the matrix above, the inverse matrix is (check for yourself)</p>
<p><span class="math display">\[ M^{-1} = \begin{pmatrix}1/3 &amp; 1/3 \\ -1/3 &amp; 2/3\end{pmatrix} \]</span></p>
<p>In general, finding the inverse of a matrix is best left to computers. However, for a 2 by 2 matrix, there is an explicit formula for an inverse:</p>
<p><span class="math display">\[
M = \begin{pmatrix} \alpha &amp; \beta \\ \gamma &amp; \delta \end{pmatrix}  \\
M^{-1} = \frac{1}{\det(M)} \begin{pmatrix} \delta &amp; -\beta \\ -\gamma &amp; \alpha \end{pmatrix}
\]</span></p>
<p>where the determinant <span class="math inline">\(\det(M) = \alpha \delta - \beta \gamma\)</span>. Note that the division by the determinant of <span class="math inline">\(M\)</span> in front of the matrix means every element of <span class="math inline">\(M\)</span> is divided by determinant (as we see in the example above, where every element is divided by 3).</p>
<p>Once we have found the inverse of a matrix, we can solve the linear equation by multiplying both sides by the inverse:</p>
<p><span class="math display">\[
M^{-1} \times M \times \vec a = V^{-1} \times \vec v \\
\vec a =  M^{-1} \times \vec v
\]</span></p>
<p>In the example above, we multiply the vector <span class="math inline">\(\vec v\)</span> by the inverse and find the solution: <span class="math inline">\((a,b) = (-2/3, 5/3)\)</span> (you can check that it works by plugging it into the original equations)</p>
<section id="invertibility-of-matrices" class="level3" data-number="7.1.1">
<h3 data-number="7.1.1" class="anchored" data-anchor-id="invertibility-of-matrices"><span class="header-section-number">7.1.1</span> invertibility of matrices</h3>
<p>It is useful to consider the geometric meaning of systems of linear equations. In two dimensions, as in the above example, each equation can be represented by a line in the plane. The solution to the two equations is the intersection of the two lines. The intersection is guaranteed to exist if the two lines are not parallel. If they are indeed parallel, then they either do not intersect at all, so there is no solution, or they overlap completely, in which case there are infinitely many solutions.</p>
<p>A similar geometric interpretation is true in higher dimensions. In three dimensions, each linear equation represents a plane, and as long as no two planes are parallel, there is only one point in which they intersect. But if two planes have the same direction, again, there is either no solution, or infinitely many (a line or plane of solutions). In higher dimensions, a solution is the intersection of <span class="math inline">\(n\)</span> hyper-planes, and again, for a unique solutions to exist, no two hyper-planes can be parallel.</p>
<p>We saw the algebraic and geometric approach to solving systems of linear equations. In the algebraic solution, we can multiply by the inverse of the matrix, but we did not specify when it exists. Algebraically speaking, this can be determined from the determinant of the matrix, as in the formula for the inverse of a 2 by 2 matrix. This is the reason for the following fundamental result:</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<p>Invertibility property: For a square (<span class="math inline">\(n\)</span> by <span class="math inline">\(n\)</span>) matrix <span class="math inline">\(M\)</span>, an inverse matrix <span class="math inline">\(M^{-1}\)</span> (also <span class="math inline">\(n\)</span> by <span class="math inline">\(n\)</span>) exists if any only if the determinant of <span class="math inline">\(M\)</span> is not zero.</p>
</div>
</div>
<p>Geometrically speaking, a determinant of zero indicates that the intersection of the lines (or hyperplanes) is not a single point or speaking mathematically, they are not linearly independent. If that is the case, as we said above, there is not unique solution to the system of equations: there are either none, or infinitely many solutions.</p>
</section>
</section>
<section id="fitting-a-line-to-data" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="fitting-a-line-to-data"><span class="header-section-number">7.2</span> Fitting a line to data</h2>
<p>One of the most common questions in data science (or any science) is to describe a relationship between two numeric variables. Often, one is seen as the potential cause and the other as the effect, and they are called the explanatory and response variables, respectively. For example, {numref}<code>fig-cancer-risk</code> plots multiple data points of the cancer risk for different types of tissues plotted on the y-axis (response) as a function of the total number of cell divisions plotted on the x-axis (explanatory).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/cancer_lin_reg.jpeg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Cancer risk (response) as a function of number of cell divisions (explanatory); Figure from &lt;https://www.science.org/doi/10.1126/science.1260825?)</figcaption><p></p>
</figure>
</div>
<p>The question is: can the relationship between the variables be described by a linear function <span class="math inline">\(y = ax + b\)</span>? And if so, how do you choose the best slope <span class="math inline">\(a\)</span> and intercept <span class="math inline">\(b\)</span>?</p>
<p>The answer is straightforward if we only have two data points: we can use the exact solution that we described in the previous section. For example, if the two data points are <span class="math inline">\((-1, -2), (5, 4)\)</span>, then the line that passes through both points must satisfy both equations below, with <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> being the slope and the intercept:</p>
<p><span class="math display">\[
-a + b = -2 \\
5a + b  = 4
\]</span></p>
<p>To find the solution for <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, we take the inverse of the matrix of coefficients <span class="math inline">\(M\)</span> and multiply it by the vector <span class="math inline">\(\vec v\)</span> on the left hand side:</p>
<p><span class="math display">\[ M = \begin{pmatrix} -1 &amp; 1 \\ 5 &amp; 1\end{pmatrix}; \vec v =  \begin{pmatrix} -2 \\ 4 \end{pmatrix}\\
M^{-1} \times \vec v  = \frac{1}{-6} \begin{pmatrix} 1 &amp; -1 \\ -5 &amp; -1 \end{pmatrix} \times  \begin{pmatrix} -2 \\ 4 \end{pmatrix} =  \begin{pmatrix} 1\\ -1 \end{pmatrix} \]</span></p>
<p>This means that a line with slope 1 and intercept -1 will pass through these two points.</p>
<p>But of course two data points is a very small amount of data to build a model. To make it just a bit more interesting, let’s add one more data point, so our data set is: <span class="math inline">\((-1, -2), (5, 4), (2,7)\)</span>. How can we find a line to fit those points?</p>
<p>Bad idea: Take two points and find a line, that is the slope and the intercept, that passes through the two. It should be clear why this is a bad idea: we are arbitrarily ignoring some of the data, while perfectly fitting two points.</p>
<p>So how do we use all the data? Let us write down the equations that a line with slope <span class="math inline">\(a\)</span> and intercept <span class="math inline">\(b\)</span> have to satisfy in order to fit our data points:</p>
<p><span class="math display">\[
-a + b = -2 \\
5a + b = 4 \\
2a + b = 7
\]</span></p>
<p>Let us write it in matrix form again:</p>
<p><span class="math display">\[  
\begin{pmatrix} -1 &amp; 1 \\ 5 &amp; 1 \\ 2 &amp; 1\end{pmatrix} \times \begin{pmatrix} a \\b\end{pmatrix} = \begin{pmatrix} -2 \\ 4 \\ 7 \end{pmatrix} \\
M \times \begin{pmatrix} a \\ b\end{pmatrix} =  \vec v
\]</span></p>
<p>This system has no exact solution, since there are three equations and only two unknowns. We need to find <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> such that they provide the <em>best fit</em> to the data, not the perfect solution. To do that, we need to define how to measure goodness of fit.</p>
<section id="minimizing-the-sum-of-residuals" class="level3" data-number="7.2.1">
<h3 data-number="7.2.1" class="anchored" data-anchor-id="minimizing-the-sum-of-residuals"><span class="header-section-number">7.2.1</span> minimizing the sum of residuals</h3>
<p>The most common approach to determine the goodness of fit is to subtract the predicted values of <span class="math inline">\(y\)</span> from the data, as follows: <span class="math inline">\(e_i = y_i - (mx_i + b)\)</span>. However, if we add it all up, the errors with opposite signs will cancel each other, giving the impression of a good fit simply if the deviations are symmetric. A more reasonable approach is to take absolute values of the deviations before adding them up. This is called the total deviation, for <span class="math inline">\(n\)</span> data points with a line fit:</p>
<p><span class="math display">\[ TD = \sum_{i=1}^n |  y_i - mx_i - b | \]</span></p>
<p>Mathematically, a better measure of total error is a sum of squared errors, which also has the advantage of adding up nonnegative values, but is known as a better measure of the distance between the fit and the data (think of Euclidean distance, which is also a sum of squares):</p>
<p><span class="math display">\[ SSE = \sum_{i=1}^n ( y_i - mx_i - b )^2 \]</span></p>
<p>To calculate the best-fit slope and intercept, we first need to define the variance and covariance of a data set:</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <em>variance</em> of a data set <span class="math inline">\(X\)</span> with <span class="math inline">\(n\)</span> data points is the following sum, where <span class="math inline">\(\bar X\)</span> is the mean of the data:</p>
<p><span class="math display">\[
Var(X) = \frac{1}{n-1} \sum_{i=1}^n (\bar X - x_i)^2
\]</span> The covariance of a data set of pairs of values <span class="math inline">\((X,Y)\)</span> is the sum of the products of the corresponding deviations from their respective means:</p>
<p><span class="math display">\[ Cov(X,Y) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar X) (y_i -\bar Y) \]</span></p>
</div>
</div>
<p>Intuitively, this means that if two variables tend to deviate in the same direction from their respective means, they have a positive covariance, and if they tend to deviate in opposite directions from their means, they have a negative covariance. In the intermediate case, if sometimes they deviate together and other times they deviate in opposition, the covariance is small or zero. For instance, the covariance between two independent random variables is zero.</p>
<p>It should come as no surprise that the slope of the linear regression depends on the covariance, that is, the degree to which the two variables deviate together from their means. If the covariance is positive, then for larger values of <span class="math inline">\(x\)</span> the corresponding <span class="math inline">\(y\)</span> values tend to be larger, which means the slope of the line is positive. Conversely, if the covariance is negative, so is the slope of the line. And if the two variables are independent, the slope has to be close to zero. The actual formula for the slope of the linear regression is:</p>
<p><span class="math display">\[
a = \frac{Cov(X,Y)}{Var(X)}
\]</span></p>
<p>To find the intercept of the linear regression, we make use of one other property of the best fit line: in order for it to minimize the SSE, it must pass through the point <span class="math inline">\((\bar X, \bar Y)\)</span>. Again, I will not prove this, but note that the point of the two mean values is the central point of the “cloud” of points in the scatterplot, and if the line missed that central point, the deviations will be larger. Assuming that is the case, we have the following equation for the line: <span class="math inline">\(\bar Y = a\bar X + b\)</span>, which we can solve for <span class="math inline">\(b\)</span>:</p>
<p><span class="math display">\[
b = \bar Y - \frac{Cov(X,Y) \bar X}{Var(X)}
\]</span></p>
<p>The parameters of the best-fit line can be calculated from the means, variances, and covariance of the two variable data set. But where did the formulas come from?</p>
<p>We want find the slope and intercept (<span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>) which result in the lowest sum of squared errors. This approach is generally known as least squares fitting, and in the case of fitting a line, it is called linear <em>regression</em>. One way to find the values that minimize the sum of squared errors is to find the derivatives of SSE with respect to <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> and set them to 0:</p>
<p><span class="math display">\[
\frac {\partial SSE}{\partial a}  = \sum_{i=1}^n -2x_i( y_i - ax_i - b ) = 0 \\
\frac {\partial SSE}{\partial b}  = \sum_{i=1}^n -2( y_i - ax_i - b ) = 0
\]</span></p>
<p>Re-write this with the <span class="math inline">\(y_i\)</span>s on the right hand side:</p>
<p><span class="math display">\[
a \sum_{i=1}^n  x_i^2 +  b \sum_{i=1}^n x_i  = \sum_{i=1}^n x_i y_i  \\
a \sum_{i=1}^n  x_i +  b \sum_{i=1}^n 1  = \sum_{i=1}^n  y_i
\]</span></p>
<p>This is now a linear system of equations, just as we started with. Turns out, there is compact way of representing this equation in matrix notation. Using the notation from the example above, let the matrix <span class="math inline">\(M\)</span> contain a column of <span class="math inline">\(x\)</span> values from the data, and a column of ones, and the vector <span class="math inline">\(\vec y\)</span> contain a column of <span class="math inline">\(y\)</span> values of the data:</p>
<p><span class="math display">\[
M = \begin{pmatrix} x_1 &amp; 1 \\... &amp; ... \\x_n &amp; 1\end{pmatrix}; \; \vec y = \begin{pmatrix} y_1 \\... \\y_n \end{pmatrix}
\]</span></p>
<p>Then the equations above can be written as the following linear algebra equation, and solved using matrix inverse:</p>
<p><span class="math display">\[
M^t \times M \times \begin{pmatrix} a \\ b\end{pmatrix}  = M^t \times \vec y \\
\begin{pmatrix} a \\ b \end{pmatrix}  = (M^t \times M)^{-1} \times M^t \times  \vec y \]</span></p>
<p>There is a linear algebra fact that the 2 by 2 matrix <span class="math inline">\(M^t \times M\)</span> is invertible so long as the columns of <span class="math inline">\(M\)</span> are linearly independent. In this case this means as long as the <span class="math inline">\(x\)</span> values of the data are not all the same, we can find a least-squares linear fit to a set of <span class="math inline">\(n\)</span> data points. If you write down the solution for <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> as sums of all the components, you will obtain the formulas that were presented above.</p>
<p>One essential measure of the quality of linear regression is correlation, which is a measure of how much variation in one random variable corresponds to variation in the other. If this sounds very similar to the description of covariance, it’s because they are closely related. Essentially, correlation is normalized covariance, made to range between -1 and 1. Here is the definition:</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<p>The (linear or Pearson) correlation of a data set of pairs of data values <span class="math inline">\((X,Y)\)</span> is:</p>
<p><span class="math display">\[ r = \frac{Cov(X,Y)}{\sqrt{{Var(X)}{Var(Y)}}} =  \frac{Cov(X,Y)}{\sigma_X \sigma_Y}\]</span></p>
</div>
</div>
<p>If the two variables are identical, <span class="math inline">\(X=Y\)</span>, then the covariance becomes its variance <span class="math inline">\(Cov(X,Y) = Var(X)\)</span> and the denominator also becomes the variance, and the correlation is 1. This is also true if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are scalar multiples of each other, as you can see by plugging in <span class="math inline">\(X= cY\)</span> into the covariance formula. The opposite case if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are diametrically opposite, <span class="math inline">\(X = -cY\)</span>, which has the correlation coefficient of -1. All other cases fall in the middle, neither perfect correlation nor perfect anti-correlation. The special case if the two variables are independent, and thus their covariance is zero, has the correlation coefficient of 0.</p>
<p>This gives a connection between correlation and slope of linear regression:</p>
<p><span class="math display">\[
a = r \frac{\sigma_Y}{\sigma_X}
\]</span></p>
<p>Whenever linear regression is reported, one always sees the values of correlation <span class="math inline">\(r\)</span> and squared correlation <span class="math inline">\(r^2\)</span> displayed. The reason for this is that <span class="math inline">\(r^2\)</span> has the meaning of the the fraction of the variance of the dependent variable <span class="math inline">\(Y\)</span> explained by the linear regression <span class="math inline">\(Y=aX+b\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Correlation_examples.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Correlation coefficient does not tell the whole story when it comes to describing the relationship between two variables; multiple scatterplots of generated data with correlation coefficient r shown above.<a href="http://en.wikipedia.org/wiki/File:Correlation_examples2.svg" class="uri">http://en.wikipedia.org/wiki/File:Correlation_examples2.svg</a></figcaption><p></p>
</figure>
</div>
<p>There are, as usual, a couple of cautions about relying on the correlation coefficient First, just because there is no linear relationship, does not mean that there is no other relationship. {numref}<code>fig-corr-examples</code> shows some examples of scatterplots and their corresponding correlation coefficients. What it shows is that while a formless blob of a scatterplot will certainly have zero correlation, so will other scatterplots in which there is a definite relationship (e.g.&nbsp;a circle, or a X-shape). The point is that <strong>correlation is always a measure of the linear relationship between variables</strong>.</p>
<p>Second cautionary tale is well known, as that is the danger of equating correlation with a causal relationship. There are numerous examples of scientists misinterpreting a coincidental correlation as meaningful, or deeming two variables that have a common source as causing one another. It cannot be repeated often enough that one must be careful when interpreting correlation: a weak one does not mean there is no relationship, and a strong one does not mean that one variable causes the variation in the other.</p>
</section>
</section>
<section id="assumptions-of-linear-regression" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="assumptions-of-linear-regression"><span class="header-section-number">7.3</span> assumptions of linear regression</h2>
<p>The simple formulas for slope, intercept, and standard deviation are only valid under certain conditions. The classic linear regression presented above relies on the following assumptions:</p>
<ul>
<li>the two variables have a linear relationship</li>
<li>the measurements are all independent of each other</li>
<li>there is no noise in the measurements of the independent variable</li>
<li>the noise in the measurements of the dependent variable is normally distributed with the same variance</li>
</ul>
<p>In reality, each data measurement has a random component, that we can call noise, resulting from experimental error, environmental variation, etc, and different measurements may have different levels of noise (standard deviation). One can estimate the error for a measurement, for instance by repeating the experiment several times, and estimating the standard deviation of the measurement random variable (we will not get into how to do this until the third quarter). It is important to account for this uncertainty in the data, since a measurement which is all over the place must carry less weight than one which is solid. A proper mathematical way of doing this is by defining a different function to measure the goodness of fit, known as the chi-squared function:</p>
<p><span class="math display">\[
\chi^2 = \sum_{i=1}^n \frac{( y_i - ax_i - b )^2 }{\sigma_i^2}
\]</span> where <span class="math inline">\(\sigma_i\)</span> is the standard deviation of the <span class="math inline">\(i\)</span>-th data point. Given all this information, we can find a solution analogous to the one found in the previous section. The only modification is to divide the matrices by the standard deviation <span class="math inline">\(\sigma_i\)</span>:</p>
<p><span class="math display">\[
M = \begin{pmatrix} x_1/\sigma_i &amp; 1/\sigma_1 \\... &amp; ... \\x_n/\sigma_n &amp; 1/\sigma_n \end{pmatrix} \\
\vec y = \begin{pmatrix} y_1/\sigma_1 \\... \\y_n /\sigma_n \end{pmatrix}
\]</span></p>
<p>Then the least squares solution is found by the same formula as above, but here we have accounted for the experimental uncertainty:</p>
<p><span class="math display">\[
\begin{pmatrix} a \\b\end{pmatrix}  = (M^t \times M)^{-1} \times M^t \times \vec y
\]</span></p>
</section>
<section id="linear-least-squares-for-polynomial-fitting" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="linear-least-squares-for-polynomial-fitting"><span class="header-section-number">7.4</span> linear least squares for polynomial fitting</h2>
<p>Fitting data sets is not restricted to linear functions. One simple extension is extension is to higher degree polynomials. Let us consider a quadratic function: <span class="math inline">\(y = ax^2 + bx + c\)</span>. By analogy with the equations for fitting a linear function, we have a set of <span class="math inline">\(n\)</span> equations, one for each data point:</p>
<p><span class="math display">\[
ax_1^2 + bx_1 + c  =  y_1 \\
ax_2^2 + bx_2 + c  =  y_2 \\
.... \\
ax_n^2 + bx_n + c  =  y_n
\]</span></p>
<p>Thus, we can define the matrix <span class="math inline">\(M\)</span> for the least-squares quadratic fit, along with the same vector <span class="math inline">\(\vec y\)</span> as follows:</p>
<p><span class="math display">\[
M = \begin{pmatrix} x_1^2 &amp; x_1 &amp; 1 \\... &amp; ... &amp; ... \\x_n^2 &amp; x_n &amp; 1\end{pmatrix}; \; \vec y = \begin{pmatrix} y_1 \\... \\ y_n\end{pmatrix}
\]</span></p>
<p>and find the best fit parameters for the quadratic function:</p>
<p><span class="math display">\[
\begin{pmatrix}a \\b \\ c \end{pmatrix}  = (M^t \times M)^{-1} \times M^t \times \vec y
\]</span></p>
<p>It is straightforward to extend this to higher order polynomials, just by adding columns of higher powers of <span class="math inline">\(x\)</span> data to the matrix <span class="math inline">\(M\)</span>. The basic structure of the solution remains the same.</p>
<p>Another important concern is about the appropriate number of parameters in a fit for a particular data set. It is clear that adding more parameters results in better fit, but at some point the number of parameters is too large, and “over-fitting” becomes as issue. Obviously, if one uses the same number of parameters as data points, one can obtain a perfect fit that has little predictive power - it just matches the given data. Deciding at what point adding more parameters is not productive is a difficult question, which can be addressed by various statistical methods that are outside of the scope of the course.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./ch6_matrix_mult.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Matrix multiplication and population models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./ch8_LinReg_python.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Linear regression in Python</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>